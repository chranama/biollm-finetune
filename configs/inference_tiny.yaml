model:
  path: "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
  # No adapters for local smoke; comment-in if you trained a tiny adapter
  # adapter_path: "results/ckpts/tiny_adapter"
  load_4bit: false
  load_8bit: false
  bf16: false            # keep false on Mac; mixed-precision is finicky
  max_length: 384

inference:
  batch_size: 1
  max_input_length: 384
  max_new_tokens: 128
  do_sample: false
  temperature: 1.0
  top_p: 1.0
  num_beams: 1
  log_every: 5
  sanitize_logits: true   # turn off if you want raw behavior

data:
  include_snippets: true

prompts: {}  # use defaults